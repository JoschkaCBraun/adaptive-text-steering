{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create topic strings\n",
    "For each topic create multiple strings that encode the topic well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'evaluate'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mevaluation_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_topic_words\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mread_and_load_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_lda\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mconfig\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TOPICS_CONFIG\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/0_Studium/0_ML_Master/0_current/nlp_research_project/research-project/notebooks/newts_topics/../../src/utils/evaluation_utils.py:18\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrouge_score\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m rouge_scorer\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mbert_score\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BERTScorer\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mevaluate\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# Local imports\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'evaluate'"
     ]
    }
   ],
   "source": [
    "from src.utils.evaluation_utils import get_topic_words\n",
    "from src.utils.read_and_load_utils import load_lda\n",
    "from config import TOPICS_CONFIG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = load_lda()\n",
    "# Warning \"WARNING:root:random_state not set so using default value\" is inconsequential for inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the NEWTS dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.read_and_load_utils import read_dataset\n",
    "\n",
    "# Load the NEWTS dataset\n",
    "newts_train = read_dataset(\"newts_train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newts_train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newts_train['phrases1'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newts_train['sentences1'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of different topics (tid1 or tid2) in the dataset\n",
    "relevant_tids = set(map(int, newts_train['tid1'].unique())).union(set(map(int, newts_train['tid2'].unique())))\n",
    "print(\"Number of different topics in the dataset: \", len(relevant_tids))\n",
    "print(\"Topic IDs: \", relevant_tids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting topic strings\n",
    "For each of the 50 topics that appead in the NEWTS train dataset, create topic strings.\n",
    "As a datastructure we use a dictionary and use the relevant_tids as the keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_strings = dict()\n",
    "for tid in relevant_tids:\n",
    "    topic_strings[tid] = dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic string: topic words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topic_words = TOPICS_CONFIG['num_topic_words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tid in relevant_tids:\n",
    "    topic_words = get_topic_words(lda=lda, tid=tid, num_topic_words=num_topic_words)\n",
    "    # concatenate the words into a single string\n",
    "    topic_strings[tid]['topic_words'] = \" \".join(topic_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Example of topic words for topic 175: \", topic_strings[175]['topic_words'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic string: topic phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# while not all relevant_tids have been seen, iterate over the dataset and use the phrases as topic_strings[tid][topic_phrases]\n",
    "missing_tids = set(relevant_tids)\n",
    "for index, row in newts_train.iterrows():\n",
    "    tid1 = row['tid1']\n",
    "    tid2 = row['tid2']\n",
    "    if tid1 in missing_tids:\n",
    "        topic_strings[tid1]['topic_phrases'] = row['phrases1']\n",
    "        missing_tids.remove(tid1)\n",
    "    if tid2 in missing_tids:\n",
    "        topic_strings[tid2]['topic_phrases'] = row['phrases2']\n",
    "        missing_tids.remove(tid2)\n",
    "    if len(missing_tids) == 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Example of topic phrases for topic 175: \", topic_strings[175]['topic_phrases'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic string: topic description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# while not all relevant_tids have been seen, iterate over the dataset and use the sentences as topic_strings[tid][topic_description]\n",
    "missing_tids = set(relevant_tids)\n",
    "for index, row in newts_train.iterrows():\n",
    "    tid1 = row['tid1']\n",
    "    tid2 = row['tid2']\n",
    "    if tid1 in missing_tids:\n",
    "        topic_strings[tid1]['topic_description'] = row['sentences1']\n",
    "        missing_tids.remove(tid1)\n",
    "    if tid2 in missing_tids:\n",
    "        topic_strings[tid2]['topic_description'] = row['sentences2']\n",
    "        missing_tids.remove(tid2)\n",
    "    if len(missing_tids) == 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Example of topic description for topic 175: \", topic_strings[175]['topic_description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the topic strings to a file under the data folder in the topic_vectors_data folder\n",
    "# the data folder is two levels up from the current folder\n",
    "import os\n",
    "import json\n",
    "\n",
    "with open('../../data/topic_vectors_data/topic_strings.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(topic_strings, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use topic focussed summaries to generate topic vectors\n",
    "Instead of using direct descriptions of the topic, use the summaries that focus on the topic.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each relevant tid, count the number of summaries that focus on that topic. \n",
    "# Use the tid1 and tid2 columns to count the number of summaries that focus on that topic.\n",
    "topic_counts = dict()\n",
    "for tid in relevant_tids:\n",
    "    topic_counts[tid] = 0\n",
    "\n",
    "for index, row in newts_train.iterrows():\n",
    "    tid1 = row['tid1']\n",
    "    tid2 = row['tid2']\n",
    "    topic_counts[tid1] += 1\n",
    "    topic_counts[tid2] += 1\n",
    "\n",
    "print(\"Number of summaries that focus on each topic: \", topic_counts)\n",
    "print(\"Min and max number of summaries that focus on a topic: \", min(topic_counts.values()), max(topic_counts.values()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research_project_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
