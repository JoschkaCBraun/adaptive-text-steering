'''
extrinsic_quality_scorer.py

This script evaluates the similarity between a single generated text and a single
reference text using metrics like ROUGE and BERTScore, loading models only once.
'''

# Standard library imports
import sys
import logging
from typing import Dict

# --- Third-party imports ---
from rouge_score import rouge_scorer
from bert_score import BERTScorer

# --- Local imports ---
from src.utils import get_device
from config.score_and_plot_config import ScoreAndPlotConfig

# Set up logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# --- Text Similarity Evaluator Class ---

class TextSimilarityEvaluator:
    """
    Loads similarity scoring models once and evaluates individual text pairs.

    Calculates ROUGE (R1, R2, RL F1) and BERTScore (F1) between a
    generated text and a reference text. Reads BERTScore model name from config.
    Uses get_device from src.utils.
    """
    def __init__(self,
                 config: ScoreAndPlotConfig):
        """
        Initializes the evaluator by loading the necessary models based on config.

        Args:
            config (ScoreAndPlotConfig): Configuration object containing settings like
                                         bertscore_model_name.

        Raises:
            AttributeError: If the config object is missing required attributes (e.g., bertscore_model_name).
            Exception: If model loading fails.
        """
        logger.info("Initializing TextSimilarityEvaluator...")
        self.config = config

        # Validate required config attributes
        if not hasattr(self.config, 'bertscore_model_name'):
             raise AttributeError("Configuration object must have attribute 'bertscore_model_name'.")

        self.device = get_device()
        # 1. Initialize ROUGE Scorer (Stemmer hardcoded to True)
        logger.info("Initializing ROUGE scorer (use_stemmer=True)...")
        try:
            self.rouge_scorer = rouge_scorer.RougeScorer(
                ['rouge1', 'rouge2', 'rougeL'],
                use_stemmer=True 
            )
            logger.info("ROUGE scorer initialized successfully.")
        except Exception as e:
            logger.error(f"Failed to initialize ROUGE scorer: {e}", exc_info=True)
            raise

        # 2. Initialize BERTScorer (Model name from config)
        bert_model = self.config.bertscore_model_name
        logger.info(f"Initializing BERTScorer (model: {bert_model}, device: {self.device})...")
        try:
            self.bert_scorer = BERTScorer(
                model_type=bert_model,
                lang='en',
                rescale_with_baseline=False,
                device=self.device.type
            )
            logger.info("BERTScorer initialized successfully.")
        except Exception as e:
            logger.error(f"Failed to initialize BERTScorer with model {bert_model}: {e}", exc_info=True)
            raise

        logger.info("TextSimilarityEvaluator initialized successfully.")


    def get_similarity_scores(self, generated_text: str, reference_text: str) -> Dict[str, float]:
        """
        Calculates similarity scores between a single generated text and a reference text.

        Args:
            generated_text (str): The text generated by a model or process.
            reference_text (str): The ground truth or target text.

        Returns:
            Dict[str, float]: A dictionary containing similarity scores:
                              {'rouge1': ROUGE-1 F1,
                               'rouge2': ROUGE-2 F1,
                               'rougeL': ROUGE-L F1,
                               'bert_f1': BERTScore F1}
                              Returns scores as 0.0 if calculation fails for a metric.
        """
        scores = {
            'rouge1': 0.0,
            'rouge2': 0.0,
            'rougeL': 0.0,
            'bert_f1': 0.0
        }

        if not generated_text or not reference_text:
             logger.warning("Received empty generated or reference text. Returning zero scores.")
             return scores

        # 1. Calculate ROUGE Scores
        try:
            rouge_results = self.rouge_scorer.score(target=reference_text, prediction=generated_text)
            scores['rouge1'] = rouge_results['rouge1'].fmeasure
            scores['rouge2'] = rouge_results['rouge2'].fmeasure
            scores['rougeL'] = rouge_results['rougeL'].fmeasure
        except Exception as e:
            logger.error(f"Failed to calculate ROUGE scores for text pair. Error: {e}", exc_info=False)
            # Keep default 0.0 scores

        # 2. Calculate BERTScore
        try:
             _, _, f1 = self.bert_scorer.score(cands=[generated_text], refs=[reference_text])
             scores['bert_f1'] = f1.item() # Get the single F1 score from the tensor
        except Exception as e:
             logger.error(f"Failed to calculate BERT scores for text pair. Error: {e}", exc_info=False)
             # Keep default 0.0 score

        return scores

# --- Main Execution Example with Test Cases ---

def main():
    '''Demonstrates initializing and using the TextSimilarityEvaluator with config.'''
    logger.info("Starting text similarity evaluation script.")

    # --- Configuration Setup ---
    # Directly instantiate the config object - assumes ScoreAndPlotConfig was imported
    config = ScoreAndPlotConfig()
    logger.info(f"Using Configuration: BERTScore Model={getattr(config, 'bertscore_model_name', 'N/A')}")

    # --- Initialize Evaluator (Loads Models ONCE using config) ---
    logger.info("--- Initializing Evaluator ---")
    try:
        # Pass the config object during initialization
        # device_override can be set here e.g. evaluator = TextSimilarityEvaluator(config=config, device_override='cpu')
        evaluator = TextSimilarityEvaluator(config=config)
        logger.info("--- Evaluator Initialized Successfully ---")
    except (AttributeError, Exception) as e:
        logger.error(f"Fatal error during evaluator initialization: {e}", exc_info=True)
        sys.exit(1)

    # --- Test Cases (Same as before) ---
    test_cases = [
        {
            "id": "Identical Texts",
            "generated": "The quick brown fox jumps over the lazy dog.",
            "reference": "The quick brown fox jumps over the lazy dog."
        },
        {
            "id": "Completely Different",
            "generated": "Apples are red.",
            "reference": "The weather is nice today."
        },
        {
            "id": "Partial Overlap",
            "generated": "The quick brown fox is fast.",
            "reference": "A quick brown fox jumps over the lazy dog."
        },
        {
            "id": "Synonyms / Paraphrase",
            "generated": "The speedy brown fox leaped above the sleeping canine.",
            "reference": "The quick brown fox jumps over the lazy dog."
        },
        {
            "id": "Different Length",
            "generated": "The fox jumps.",
            "reference": "The quick brown fox jumps over the lazy dog near the river bank."
        },
        {
            "id": "Empty Generated",
            "generated": "",
            "reference": "This is the reference text."
        },
         {
            "id": "Empty Reference",
            "generated": "This is the generated text.",
            "reference": ""
        }
    ]

    logger.info("--- Running Test Cases ---")
    for case in test_cases:
        logger.info(f"\n--- Test Case: {case['id']} ---")
        logger.info(f"  Generated: '{case['generated']}'")
        logger.info(f"  Reference: '{case['reference']}'")

        # Get scores using the *same* evaluator instance
        similarity_scores = evaluator.get_similarity_scores(
            generated_text=case['generated'],
            reference_text=case['reference']
        )

        print(f"  Scores:")
        for name, score in similarity_scores.items():
            print(f"    {name}: {score:.4f}")

    logger.info("\n--- Test Cases Finished ---")
    logger.info("Text similarity evaluation script finished.")


if __name__ == "__main__":
    main()